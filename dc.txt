services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 5s
      timeout: 3s
      retries: 30

  postgres-mlflow-init:
    image: postgres:13
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      PGPASSWORD: airflow
    command:
      - bash
      - -lc
      - |
        set -e
        until pg_isready -h postgres -U airflow -d airflow; do
          echo "Waiting for Postgres..."
          sleep 2
        done

        echo "Postgres is ready. Checking if mlflow DB exists..."
        psql -h postgres -U airflow -d airflow -tc "SELECT 1 FROM pg_database WHERE datname='mlflow'" | grep -q 1 || \
          psql -h postgres -U airflow -d airflow -c "CREATE DATABASE mlflow"

        echo "Done."
    restart: "no"

  minio:
    image: minio/minio:RELEASE.2024-01-13T07-53-03Z
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command:
      - server
      - /data
      - --console-address
      - ":9001"
    volumes:
      - minio_data:/data
    ports:
      - "9000:9000"
      - "9001:9001"

  minio-init:
    image: minio/mc:RELEASE.2024-01-13T08-44-48Z
    depends_on:
      minio:
        condition: service_started
    entrypoint: ["/bin/sh"]
    command:
      - -ec
      - |
        echo "Waiting for MinIO..."
        for i in $(seq 1 60); do
          mc alias set local http://minio:9000 minioadmin minioadmin >/dev/null 2>&1 && break
          sleep 2
        done

        echo "Creating bucket (if missing): mlflow-artifacts"
        mc mb -p local/mlflow-artifacts || true

        echo "Buckets:"
        mc ls local
    restart: "no"

  mlflow-db-init:
    image: python:3.11-slim
    depends_on:
      postgres:
        condition: service_healthy
      postgres-mlflow-init:
        condition: service_completed_successfully
      minio-init:
        condition: service_completed_successfully
    environment:
      PIP_NO_CACHE_DIR: "1"
      MLFLOW_BACKEND_STORE_URI: "postgresql+psycopg2://airflow:airflow@postgres:5432/mlflow"
      MLFLOW_DEFAULT_ARTIFACT_ROOT: "s3://mlflow-artifacts"
    command:
      - bash
      - -lc
      - |
        set -euo pipefail

        apt-get update
        apt-get install -y --no-install-recommends bash ca-certificates
        rm -rf /var/lib/apt/lists/*

        pip install --no-cache-dir "mlflow>=2.0.0,<3.0.0" psycopg2-binary boto3

        echo "Waiting for Postgres (mlflow DB)..."
        python - <<'PY'
        import os, time
        import psycopg2

        uri = os.environ["MLFLOW_BACKEND_STORE_URI"].replace("postgresql+psycopg2://", "postgresql://")
        for _ in range(60):
            try:
                psycopg2.connect(uri).close()
                print("Postgres ready for MLflow")
                break
            except Exception as e:
                print("Waiting...", e)
                time.sleep(2)
        else:
            raise SystemExit("Postgres not ready in time")
        PY

        echo "Initializing MLflow tables + schema..."
        python - <<'PY'
        import os
        from mlflow.store.tracking.sqlalchemy_store import SqlAlchemyStore

        backend = os.environ["MLFLOW_BACKEND_STORE_URI"]
        artifact_root = os.environ["MLFLOW_DEFAULT_ARTIFACT_ROOT"]

        SqlAlchemyStore(backend, artifact_root)
        print("MLflow DB initialized")
        PY
    restart: "no"

  mlflow:
    image: python:3.11-slim
    container_name: mlflow
    depends_on:
      mlflow-db-init:
        condition: service_completed_successfully
      minio-init:
        condition: service_completed_successfully
    working_dir: /app
    environment:
      PIP_NO_CACHE_DIR: "1"
      MLFLOW_S3_ENDPOINT_URL: "http://minio:9000"
      AWS_ACCESS_KEY_ID: "minioadmin"
      AWS_SECRET_ACCESS_KEY: "minioadmin"
      AWS_DEFAULT_REGION: "us-east-1"
      MLFLOW_BACKEND_STORE_URI: "postgresql+psycopg2://airflow:airflow@postgres:5432/mlflow"
      MLFLOW_DEFAULT_ARTIFACT_ROOT: "s3://mlflow-artifacts"
    ports:
      - "5001:5000"
    command:
      - bash
      - -lc
      - |
        set -euo pipefail

        apt-get update
        apt-get install -y --no-install-recommends bash ca-certificates
        rm -rf /var/lib/apt/lists/*

        pip install --no-cache-dir "mlflow>=2.0.0,<3.0.0" psycopg2-binary boto3

        mlflow server --host 0.0.0.0 --port 5000 \
          --backend-store-uri "$$MLFLOW_BACKEND_STORE_URI" \
          --default-artifact-root "$$MLFLOW_DEFAULT_ARTIFACT_ROOT"

  airflow-init:
    build: ./airflow
    depends_on:
      postgres:
        condition: service_healthy
      postgres-mlflow-init:
        condition: service_completed_successfully
      mlflow:
        condition: service_started
      minio:
        condition: service_started
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: "zmV0NxiEfoEPwY_4WP-D0Qe2fT4RzytxbxtucLA7OHw="
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
    command:
      - bash
      - -lc
      - |
        set -e
        airflow db migrate
        airflow users create \
          --username admin --password admin \
          --firstname Sai --lastname Kumar \
          --role Admin --email vishukings@gmail.com || true
    restart: "no"

  airflow-webserver:
    build: ./airflow
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
      mlflow:
        condition: service_started
      minio:
        condition: service_started
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: "zmV0NxiEfoEPwY_4WP-D0Qe2fT4RzytxbxtucLA7OHw="
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "True"
      AIRFLOW__WEBSERVER__SECRET_KEY: "supersecretkey"
      AIRFLOW__WEBSERVER__AUTH_BACKEND: "airflow.api.auth.backend.session"
      AIRFLOW__API__AUTH_BACKEND: "airflow.api.auth.backend.basic_auth"
      MLFLOW_TRACKING_URI: "http://mlflow:5000"
      MLFLOW_S3_ENDPOINT_URL: "http://minio:9000"
      AWS_ACCESS_KEY_ID: "minioadmin"
      AWS_SECRET_ACCESS_KEY: "minioadmin"
      AWS_DEFAULT_REGION: "us-east-1"
      PLATFORM_ENV: "dev"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
    ports:
      - "8080:8080"
    command:
      - webserver

  airflow-scheduler:
    build: ./airflow
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
      mlflow:
        condition: service_started
      minio:
        condition: service_started
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: "zmV0NxiEfoEPwY_4WP-D0Qe2fT4RzytxbxtucLA7OHw="
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      MLFLOW_TRACKING_URI: "http://mlflow:5000"
      MLFLOW_S3_ENDPOINT_URL: "http://minio:9000"
      AWS_ACCESS_KEY_ID: "minioadmin"
      AWS_SECRET_ACCESS_KEY: "minioadmin"
      AWS_DEFAULT_REGION: "us-east-1"
      PLATFORM_ENV: "dev"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
    command:
      - scheduler

  jenkins:
    build: ./jenkins
    user: root
    ports:
      - "8081:8080"
    environment:
      JENKINS_OPTS: "--prefix=/jenkins"
    volumes:
      - jenkins_home:/var/jenkins_home
      - ./airflow/dags:/shared/airflow_dags:ro
      - ./:/workspace
    working_dir: /workspace

volumes:
  postgres_data:
  minio_data:
  jenkins_home:
  airflow_logs:
  airflow_plugins: